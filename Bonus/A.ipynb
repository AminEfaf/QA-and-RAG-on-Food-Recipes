{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8463a8aa29734531966b9a61405e7733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51e82a85da29423a88035a5201ca0f23",
              "IPY_MODEL_8822d84d77694cac94eae283024108e2",
              "IPY_MODEL_a2dbb3db8e4246648f7261b90b2f8952"
            ],
            "layout": "IPY_MODEL_d481586033f2423990c05f87639d8140"
          }
        },
        "51e82a85da29423a88035a5201ca0f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f978a47b177843a3a95a067974bd4201",
            "placeholder": "​",
            "style": "IPY_MODEL_6968c2ff17174f2caab1f9079db07628",
            "value": "Computing widget examples:   0%"
          }
        },
        "8822d84d77694cac94eae283024108e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e67ccf62f454da2bcdd1205fad760ca",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a465c88b4d64fe6a449a134fdf3c56f",
            "value": 1
          }
        },
        "a2dbb3db8e4246648f7261b90b2f8952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68bfdab9a27b431f8b57dd50adc4168e",
            "placeholder": "​",
            "style": "IPY_MODEL_891d570cc3bc40c48739f42d0da222b7",
            "value": " 0/1 [00:00&lt;?, ?example/s]"
          }
        },
        "d481586033f2423990c05f87639d8140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "f978a47b177843a3a95a067974bd4201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6968c2ff17174f2caab1f9079db07628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e67ccf62f454da2bcdd1205fad760ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a465c88b4d64fe6a449a134fdf3c56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68bfdab9a27b431f8b57dd50adc4168e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "891d570cc3bc40c48739f42d0da222b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b44444c"
      },
      "source": [
        "1.  **Install necessary packages:** Install libraries for transformers, sentence embeddings, FAISS for vector search, and document processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sle3E0LhVopN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch python-docx faiss-cpu sentence-transformers unsloth nltk scikit-learn pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "001cfd2a"
      },
      "source": [
        "2.  **Load and process the document:** Load the DOCX file and extract the text content."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import docx\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Path to the DOCX file\n",
        "docx_path = \"/content/Guilan-Food.docx\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(docx_path):\n",
        "    raise FileNotFoundError(f\"DOCX file not found at {docx_path}. Please upload it.\")\n",
        "\n",
        "# Extract text from the DOCX file\n",
        "print(\"Extracting text from DOCX...\")\n",
        "doc = docx.Document(docx_path)\n",
        "text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
        "print(f\"Extracted {len(text)} characters.\")"
      ],
      "metadata": {
        "id": "_hkj05H1s_qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d5a7136"
      },
      "source": [
        "3.  **Chunk the text:** Split the extracted text into smaller, overlapping chunks to facilitate embedding and retrieval."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=150, overlap=20):\n",
        "    \"\"\"\n",
        "    Splits text into overlapping chunks.\n",
        "    - chunk_size: number of words per chunk\n",
        "    - overlap: number of words to overlap between chunks\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "# Create chunks from the extracted text\n",
        "all_chunks = chunk_text(text, chunk_size=250, overlap=200)\n",
        "print(f\"Created {len(all_chunks)} text chunks.\")"
      ],
      "metadata": {
        "id": "2gUs_GD9s_yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ecf4cd"
      },
      "source": [
        "4.  **Generate embeddings:** Use a pre-trained sentence transformer model to create numerical representations (embeddings) of the text chunks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating embeddings for chunks...\")\n",
        "# Load a pre-trained sentence transformer model for multilingual embeddings\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
        "# Generate embeddings for all text chunks\n",
        "chunk_embeddings = embedding_model.encode(all_chunks, convert_to_numpy=True)\n",
        "\n",
        "# Normalize the embeddings\n",
        "chunk_embeddings = chunk_embeddings / np.linalg.norm(chunk_embeddings, axis=1).reshape(-1, 1)\n",
        "print(f\"Generated embeddings of shape: {chunk_embeddings.shape}\")"
      ],
      "metadata": {
        "id": "GMrdZ0aos_3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ce74d11"
      },
      "source": [
        "5.  **Build FAISS index:** Create a FAISS index to enable fast and efficient similarity search over the chunk embeddings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dimension of the embeddings\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "# Create a FAISS index for efficient similarity search using L2 distance\n",
        "index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
        "# Add the chunk embeddings to the index\n",
        "index.add(chunk_embeddings)\n",
        "print(\"FAISS index built and embeddings added.\")"
      ],
      "metadata": {
        "id": "3ocuoWs-s_-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a96b4f2c"
      },
      "source": [
        "6.  **Define retrieval function:** Create a function to retrieve the most relevant text chunks based on a given query using the FAISS index."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_contexts(query, k=3):\n",
        "    \"\"\"\n",
        "    Retrieve the top-k most relevant text chunks for a query.\n",
        "    \"\"\"\n",
        "    # Encode the query into an embedding\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    # Normalize the query embedding\n",
        "    query_embedding = query_embedding / np.linalg.norm(query_embedding)  # normalize\n",
        "    # Search the FAISS index for the k most similar chunks\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    # Return the text of the top-k chunks\n",
        "    return [all_chunks[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "uRA7zVZiuC7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ccb92b"
      },
      "source": [
        "7.  **Load Language Model:** Load a pre-trained causal language model (LLM) for generating answers. This example uses a quantized Llama-3 model for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the name of the language model to load\n",
        "model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "# Load the tokenizer for the specified model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Load the causal language model with 4-bit quantization for efficiency\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\", # Automatically detect and use available devices (like GPU)\n",
        "    torch_dtype=torch.float16 # Use float16 for reduced memory usage and faster computation\n",
        ")\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "pIdJPkxUuC9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cd35a2f"
      },
      "source": [
        "8.  **Define RAG generation function:** Create a function that combines retrieval and generation. It retrieves relevant contexts for a question and then uses the LLM to generate an answer based on those contexts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_generate(question, max_new_tokens=300):\n",
        "    \"\"\"\n",
        "    Generate an answer using RAG:\n",
        "    1. Retrieve relevant context\n",
        "    2. Inject context into prompt\n",
        "    3. Generate response with LLM\n",
        "    \"\"\"\n",
        "    # Retrieve the top-k most relevant contexts for the question\n",
        "    contexts = retrieve_contexts(question, k=3)\n",
        "    # Format the retrieved contexts for the prompt\n",
        "    context_str = \"\\n\\n\".join([f\"[Reference {i+1}]: {ctx}\" for i, ctx in enumerate(contexts)])\n",
        "\n",
        "    # Define the chat template explicitly for Llama-3 to ensure correct formatting\n",
        "    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'system' %}{{ '<|start_header_id|>system<|end_header_id|>\\n' + message['content'] + '<|eot_id|>' }}{% elif message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n' + message['content'] + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n' + message['content'] + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n' }}{% endif %}\"\n",
        "\n",
        "    # Create the messages list for the chat template\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"first of all,You should answer in persian. Based on the following documents:\\n{context_str}\\n\\nQuestion: {question}\\nAnswer concisely.\"}\n",
        "    ]\n",
        "    # Apply the chat template to format the prompt\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenize the prompt and move it to the appropriate device (GPU)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=3500).to(model.device)\n",
        "\n",
        "    # Generate a response using the loaded language model\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens, # Set the maximum number of new tokens to generate\n",
        "            temperature=0.7, # Control the randomness of the output\n",
        "            top_p=0.9, # Control the diversity of the output\n",
        "            do_sample=True, # Enable sampling\n",
        "            pad_token_id=tokenizer.eos_token_id # Set the padding token to the end-of-sequence token\n",
        "        )\n",
        "\n",
        "    # Decode the generated output\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract the answer from the full response\n",
        "    answer = full_response.split(\"[/INST]\")[-1].strip()\n",
        "    return answer"
      ],
      "metadata": {
        "id": "Gg38Dqtmw8PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86efb7e8"
      },
      "source": [
        "9.  **Test with example questions:** Use the RAG generation function to answer some example questions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of example questions in Persian\n",
        "questions = [\n",
        "    \"طرز تهیه باقلا قاتوق چیست؟\",\n",
        "    \"مواد لازم برای میرزا قاسمی چیه؟\"\n",
        "]\n",
        "\n",
        "# Generate and print answers for each question using the RAG model\n",
        "for q in questions:\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"Question: {q}\")\n",
        "    answer = rag_generate(q)\n",
        "    print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "hLmEpTecw8Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b61eda1a"
      },
      "source": [
        "10. **Advanced Evaluation (Optional):** This section includes code for fine-tuning the embedding model and evaluating the RAG system using metrics like MRR, F1 score, and Cosine Similarity. This is more advanced and may require additional setup and data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers==4.55.2 sentence-transformers datasets torch bitsandbytes python-docx nltk\n",
        "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "import unsloth  # Import unsloth before transformers\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from docx import Document\n",
        "from sentence_transformers import SentenceTransformer, losses, util\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Free GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load DOCX content\n",
        "try:\n",
        "    doc = Document('Guilan-Food.docx')\n",
        "    text = '\\n'.join([para.text for para in doc.paragraphs])\n",
        "    # Clean up text by replacing special characters and reducing multiple spaces\n",
        "    text = re.sub(r'\\·|\\•', '-', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"Please upload 'Guilan-Food.docx' to the Colab environment.\")\n",
        "\n",
        "# Questions and their corresponding ground truth answers\n",
        "questions = [\n",
        "    \"مواد لازم برای باقلا قاتوق برای 4 نفر چیست؟\",\n",
        "    \"طرز تهیه میرزا قاسمی چگونه است؟\",\n",
        "    \"مواد لازم برای کباب ترش برای 4 نفر چیست؟\",\n",
        "    \"طرز تهیه رشته خشکار چگونه است؟\",\n",
        "    \"مواد لازم برای اناربیج برای 4 نفر چیست؟\"\n",
        "]\n",
        "ground_truths = [\n",
        "    \"باقلای کشاورزی: 500 گرم (پوست کنده شده) - شوید تازه: 200 گرم (ریز خرد شده) - سیر: 3-4 حبه (رنده یا له شده) - تخم مرغ: 2-3 عدد - روغن مایع یا کره: به مقدار لازم - نمک، فلفل سیاه و زردچوبه: به مقدار لازم - آب: حدود 1 لیوان\",\n",
        "    \"برای تهیه میرزا قاسمی، ابتدا بادمجان‌ها را بشویید و روی شعله گاز، منقل یا داخل فر کبابی کنید تا پوست آن‌ها کاملاً بسوزد و داخلشان نرم شود. در حین کباب کردن، بادمجان‌ها را بچرخانید تا تمام قسمت‌ها به خوبی کباب شوند. سپس بگذارید بادمجان‌ها کمی خنک شوند و پوست سوخته آن‌ها را جدا کنید. گوشت داخل بادمجان‌ها را با چاقو ساطوری کنید، نه خیلی ریز و نه خیلی درشت. در مرحله بعد، گوجه فرنگی‌ها را بشویید و پوست آن‌ها را بگیرید. برای راحت‌تر پوست گرفتن، می‌توانید گوجه‌ها را چند ثانیه در آب جوش قرار دهید. سپس گوجه فرنگی‌ها را نگینی خرد کنید یا رنده کنید. در یک تابه مناسب، مقداری روغن یا کره بریزید و سیر له شده یا رنده شده را اضافه کنید. کمی تفت دهید تا عطر سیر بلند شود، مراقب باشید که نسوزد. سپس گوجه فرنگی‌های خرد شده یا رنده شده را به تابه اضافه کنید و تفت دهید تا آب آن کشیده شود و کمی غلیظ شود. در این مرحله، کمی نمک، فلفل سیاه و زردچوبه به آن اضافه کنید. حالا بادمجان ساطوری شده را به تابه اضافه کنید و با گوجه فرنگی و سیر مخلوط کنید. حدود 5-10 دقیقه تفت دهید تا طعم‌ها به خوبی ترکیب شوند. سپس در وسط تابه یک گودی ایجاد کرده و تخم‌مرغ‌ها را در آن بشکنید. اجازه دهید تخم‌مرغ‌ها کمی بپزند و خودشان را بگیرند، سپس آن‌ها را با مواد دیگر مخلوط کرده و هم بزنید تا کاملاً پخته و یکدست شوند. در نهایت، اجازه دهید میرزا قاسمی چند دقیقه دیگر روی حرارت ملایم بماند تا روغن بیندازد و طعم‌ها کاملاً جا بیفتند. سپس آن را در ظرف مناسب بریزید و در صورت تمایل با کمی جعفری خرد شده تزئین کنید.\",\n",
        "    \"گوشت گوساله یا گوسفند: 500 گرم (تکه شده برای کباب، معمولاً از راسته یا فیله استفاده می‌شود) - گردوی آسیاب شده: 100 گرم - رب انار ترش یا ملس: 3-4 قاشق غذاخوری (بسته به غلظت و ترشی رب) - آب انار ترش: 2-3 قاشق غذاخوری (اختیاری، برای طعم بیشتر) - سیر: 2-3 حبه (رنده یا له شده) - پیاز متوسط: 1 عدد (رنده شده و آب آن گرفته شده) - سبزیجات معطر محلی (چوچاق و خالواش) تازه یا خشک: 2-3 قاشق غذاخاری (اگر در دسترس نبود، می‌توانید از ترکیب گشنیز، جعفری و نعناع به مقدار کم استفاده کنید) - روغن زیتون یا روغن مایع: 2-3 قاشق غذاخوری - نمک و فلفل سیاه: به مقدار لازم\",\n",
        "    \"برای تهیه رشته خشکار ابتدا آرد برنج را در یک تابه با کمی کره تفت دهید تا کمی رنگ آن تغییر کند. سپس شیر را به آرد برنج اضافه کنید و هم بزنید تا مخلوطی یکدست و بدون گلوله به دست آید. حالا شکر و پودر هل را اضافه کرده و به هم زدن ادامه دهید تا شکر کاملاً حل شود. بعد از این مرحله، آب و گلاب را به مخلوط اضافه کنید و اجازه دهید مواد روی حرارت ملایم به جوش بیاید. هنگامی که مایع غلیظ شد، آن را در قالب‌های مناسب بریزید و بگذارید کمی سرد شود. در نهایت رشته خشکار را با پودر نارگیل تزئین کنید و در صورت تمایل از پسته یا بادام خرد شده نیز استفاده کنید.\",\n",
        "    \"گوشت چرخ‌کرده: 400 گرم - گردوی آسیاب شده: 200 گرم - رب انار ترش یا ملس: 3 قاشق غذاخوری - سبزی معطر (چوچاق، خالواش، گشنیز، نعناع): 200 گرم - پیاز متوسط: 1 عدد (رنده شده) - آب: حدود 3 لیوان - نمک، فلفل، زردچوبه: به مقدار لازم\"\n",
        "]\n",
        "\n",
        "# Chunking function\n",
        "def chunk_text(text, chunk_size=150, overlap=20):\n",
        "    \"\"\"\n",
        "    Splits text into overlapping chunks.\n",
        "    - chunk_size: number of words per chunk\n",
        "    - overlap: number of words to overlap between chunks\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "# Create chunks\n",
        "all_chunks = chunk_text(text, chunk_size=150, overlap=20)\n",
        "print(f\"Created {len(all_chunks)} text chunks.\")\n",
        "\n",
        "# F1 score function for evaluating generated responses\n",
        "def compute_f1(pred, gt):\n",
        "    pred_tokens = set(pred.split())\n",
        "    gt_tokens = set(gt.split())\n",
        "    common = pred_tokens & gt_tokens\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    precision = len(common) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
        "    recall = len(common) / len(gt_tokens) if len(gt_tokens) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return f1\n",
        "\n",
        "# Function to get relevant chunk indices based on ground truth text\n",
        "def get_relevant_chunks(chunks, gt, threshold=0.5):\n",
        "    gt_tokens = set(gt.split())\n",
        "    relevant = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_tokens = set(chunk.split())\n",
        "        # Calculate overlap as a ratio\n",
        "        overlap = len(gt_tokens & chunk_tokens) / len(gt_tokens) if len(gt_tokens) > 0 else 0\n",
        "        if overlap > threshold:\n",
        "            relevant.append(i)\n",
        "    return relevant\n",
        "\n",
        "# Fine-tune embedding model with more relevant training data\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "train_data = []\n",
        "# Create positive pairs (consecutive sentences)\n",
        "for i in range(len(sentences) - 1):\n",
        "    train_data.append({\"text1\": sentences[i], \"text2\": sentences[i + 1], \"label\": 1.0})\n",
        "# Create negative pairs (sentences further apart) - adjusted logic for creating negative pairs\n",
        "# A simple way is to pair sentences that are not consecutive.\n",
        "# This implementation creates pairs of sentences that are two steps apart.\n",
        "for i in range(len(sentences) - 2):\n",
        "     train_data.append({\"text1\": sentences[i], \"text2\": sentences[i + 2], \"label\": 0.0}) # Use 0.0 for non-related pairs for ContrastiveLoss\n",
        "\n",
        "# Create a Dataset from the training data\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "\n",
        "# Load the pre-trained embedding model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
        "# Define training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"fine_tuned_model\",\n",
        "    num_train_epochs=2,  # Increased for better fine-tuning\n",
        "    per_device_train_batch_size=4,\n",
        "    warmup_steps=5,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        ")\n",
        "# Define the loss function (ContrastiveLoss is suitable for positive/negative pairs)\n",
        "train_loss = losses.ContrastiveLoss(embedding_model)\n",
        "# Initialize the trainer\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss\n",
        ")\n",
        "# Start fine-tuning\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "embedding_model.save(\"fine_tuned_model\")\n",
        "\n",
        "# Load fine-tuned model, with fallback to original if loading fails\n",
        "try:\n",
        "    embedding_model = SentenceTransformer(\"fine_tuned_model\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading fine-tuned model: {e}\")\n",
        "    print(\"Falling back to the original model.\")\n",
        "    embedding_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
        "\n",
        "# Free GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load LLaMA model with reduced sequence length for memory efficiency\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length=1024,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "# Prepare the model for inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Function to retrieve top-k chunks based on query similarity\n",
        "def get_top_k_chunks(query, chunks, model, k=10):  # Increased k for more retrieval\n",
        "    if not chunks:\n",
        "        return [], []\n",
        "    # Encode the query and all chunks\n",
        "    query_embedding = model.encode(query, batch_size=1)\n",
        "    chunk_embeddings = model.encode(chunks, batch_size=1)\n",
        "    # Calculate cosine similarity between the query and each chunk\n",
        "    similarities = util.cos_sim(query_embedding, chunk_embeddings)[0]\n",
        "    if len(similarities) == 0:\n",
        "        return [], []\n",
        "    # Get the indices of the top-k most similar chunks\n",
        "    top_k_indices = np.argsort(similarities.numpy())[::-1][:k]\n",
        "    # Return the top-k chunks and their indices\n",
        "    return [chunks[i] for i in top_k_indices], top_k_indices\n",
        "\n",
        "# Response generation function\n",
        "def generate_response(query, context):\n",
        "    # Define the prompt template for the language model\n",
        "    prompt = f\"\"\"Based on the following context, only answer the question in Persian.\n",
        "Do not repeat the question or give explanations.\n",
        "Do not ask the question.\n",
        "Only return the direct answer.\n",
        "\n",
        "Context: {context[:2000]}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    # Tokenize the prompt and move to GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # Generate the response using the language model\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id)  # Reduced max_new_tokens\n",
        "    # Decode the generated output\n",
        "    decoded = tokenizer.decode(outputs[0])\n",
        "    # Extract the answer part based on the \"Answer:\" marker\n",
        "    if \"Answer:\" in decoded:\n",
        "        response = decoded.split(\"Answer:\")[-1].strip()\n",
        "    else:\n",
        "        response = decoded.strip()\n",
        "    # Clean up the response\n",
        "    response = response.replace(\"<|end_of_text|>\", \"\").strip()\n",
        "    # Print query and response for monitoring\n",
        "    print(\"====\"*40)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    # Free GPU memory after generation\n",
        "    torch.cuda.empty_cache()\n",
        "    return response\n",
        "\n",
        "# Evaluation loop\n",
        "k = 10  # Increased k to retrieve more relevant chunks\n",
        "f1_scores = []\n",
        "cosine_scores = []\n",
        "mrr_scores = []\n",
        "\n",
        "print(f\"\\n----------------------\")\n",
        "print(f\"| Overlap Chunking: |\")\n",
        "print(f\"----------------------\")\n",
        "\n",
        "# Iterate through each question and its ground truth\n",
        "for idx, (query, gt) in enumerate(zip(questions, ground_truths)):\n",
        "    # Retrieve top-k chunks\n",
        "    top_chunks, top_indices = get_top_k_chunks(query, all_chunks, embedding_model, k=k)\n",
        "    if not top_chunks:\n",
        "        print(f\"No chunks retrieved for query: {query}\")\n",
        "        # Append 0 scores if no chunks are retrieved\n",
        "        f1_scores.append(0.0)\n",
        "        cosine_scores.append(0.0)\n",
        "        mrr_scores.append(0.0)\n",
        "        continue\n",
        "    # Combine retrieved chunks into a single context string\n",
        "    context = '\\n\\n'.join(top_chunks)\n",
        "    # Generate response using the language model with the retrieved context\n",
        "    response = generate_response(query, context)\n",
        "\n",
        "    # Generation evaluation (F1 and Cosine Similarity)\n",
        "    f1 = compute_f1(response, gt)\n",
        "    # Encode the generated response and ground truth for cosine similarity calculation\n",
        "    emb_response = embedding_model.encode(response, batch_size=1)\n",
        "    emb_gt = embedding_model.encode(gt, batch_size=1)\n",
        "    cosine = util.cos_sim(emb_response, emb_gt)[0][0].item()\n",
        "\n",
        "    # Retrieval evaluation (MRR)\n",
        "    # Get the indices of truly relevant chunks based on the ground truth\n",
        "    relevant = get_relevant_chunks(all_chunks, gt)\n",
        "    mrr = 0\n",
        "    # Calculate MRR based on the rank of the first relevant retrieved chunk\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        if idx in relevant:\n",
        "            mrr = 1 / rank\n",
        "            break\n",
        "\n",
        "    # Append calculated scores\n",
        "    f1_scores.append(f1)\n",
        "    cosine_scores.append(cosine)\n",
        "    mrr_scores.append(mrr)\n",
        "\n",
        "    # Free GPU memory after each iteration\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print final average results\n",
        "print(\"\\nResults:\")\n",
        "print(f\"F1: {np.mean(f1_scores):.4f}\")\n",
        "print(f\"Cosine Similarity: {np.mean(cosine_scores):.4f}\")\n",
        "print(f\"MRR: {np.mean(mrr_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967,
          "referenced_widgets": [
            "8463a8aa29734531966b9a61405e7733",
            "51e82a85da29423a88035a5201ca0f23",
            "8822d84d77694cac94eae283024108e2",
            "a2dbb3db8e4246648f7261b90b2f8952",
            "d481586033f2423990c05f87639d8140",
            "f978a47b177843a3a95a067974bd4201",
            "6968c2ff17174f2caab1f9079db07628",
            "5e67ccf62f454da2bcdd1205fad760ca",
            "8a465c88b4d64fe6a449a134fdf3c56f",
            "68bfdab9a27b431f8b57dd50adc4168e",
            "891d570cc3bc40c48739f42d0da222b7"
          ]
        },
        "id": "-8r5sBv5E3jV",
        "outputId": "d10db148-ae83-40bc-96ab-62cadba3d5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 34 text chunks.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8463a8aa29734531966b9a61405e7733"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 179 | Num Epochs = 1 | Total steps = 45\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 135,127,808 of 135,127,808 (100.00% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [45/45 00:20, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.729500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.147600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.614600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.472300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.8.9: Fast Llama patching. Transformers: 4.55.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "----------------------\n",
            "| Overlap Chunking: |\n",
            "----------------------\n",
            "================================================================================================================================================================\n",
            "Query: مواد لازم برای باقلا قاتوق برای 4 نفر چیست؟\n",
            "Response: برای تهیه رشته خشکار، ابتدا آرد برنج را در یک تابه با کمی کره تفت دهید تا کمی رنگ آن تغییر کند. سپس شیر را به آرد برنج اضافه کنید و هم بزنید تا مخلوطی یکدست و بدون گل\n",
            "================================================================================================================================================================\n",
            "Query: طرز تهیه میرزا قاسمی چگونه است؟\n",
            "Response: برای تهیه میرزا قاسمی، ابتدا بادمجان‌ها را بشویید و روی شعله گاز، منقل یا داخل فر کبابی کنید تا پوست آن‌ها کاملاً بسوزد و داخلشان نرم شود. در حین کباب کردن، بادمجان‌ها را بچرخانید تا تمام قسمت‌ها به خوبی کباب شوند. سپس بگذارید بادمجان‌ها کمی خنک شوند و پوست سوخته آن‌ها را جدا کنید. گوشت داخل بادمجان‌ها را با چاقو ساطوری کنید، نه خیلی ریز و نه خیلی درشت. در مرحله بعد، گوجه فرنگی‌ها را بشویید و آن‌ها را هم کباب کنید. در حین کباب کردن، گوجه فرنگی‌ها را هم بچرخانید تا تمام قسمت‌ها به خوبی کباب شوند. حالا بادمجان‌ها و گوجه فرنگی‌ها را با هم مخلوط کنید و کمی روغن\n",
            "================================================================================================================================================================\n",
            "Query: مواد لازم برای کباب ترش برای 4 نفر چیست؟\n",
            "Response: گوشت بدون چربی و قورمه ای خرد شده :250 گرم پیاز رنده شده : 1 عدد سیر رنده شده یا له شده : 2-3 حبه گردوی آسیاب شده : 100 گرم رب انار ترش یا ملس : 3-4 قاشق غذاخوری (بسته به غلظت و ترشی رب) آب انار ترش : 2-3 قاشق غذاخوری (اختیاری، برای طعم بیشتر) سبزیجات معطر محلی (چوچاق و خالواش) تازه یا خشک : 2-3 قاشق غذاخوری (اگر در دسترس نبود، می‌توانید از ترکیب گشنیز، جعفری و نعناع به مقدار کم استفاده کنید) روغن زیتون یا روغن مایع : 2-3 قاشق غذاخوری نمک و فلفل سیاه : به مقدار لازم\n",
            "================================================================================================================================================================\n",
            "Query: طرز تهیه رشته خشکار چگونه است؟\n",
            "Response: برای تهیه رشته خشکار ابتدا آرد برنج را در یک تابه با کمی کره تفت دهید تا کمی رنگ آن تغییر کند. سپس شیر را به آرد برنج اضافه کنید و هم بزنید تا مخلوطی یکدست و بدون گلوله به دست آید. حالا شکر را به صورت نرم و کمی له شده است و به خوبی با نان مخلوط می‌شود.\n",
            "================================================================================================================================================================\n",
            "Query: مواد لازم برای اناربیج برای 4 نفر چیست؟\n",
            "Response: \n",
            "\n",
            "Results:\n",
            "F1: 0.4088\n",
            "Cosine Similarity: 0.9099\n",
            "MRR: 0.5167\n"
          ]
        }
      ]
    }
  ]
}